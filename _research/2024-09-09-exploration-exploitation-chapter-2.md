---
title:  "Exploration and Explotation (Multi-armed Bandits) - Chapter 2 - Sutton and Barto"
date:   2024-09-09
collection: research
permalink: /research/chapter2_SB
---
This lecture focused on the challenge of balancing exploration and exploitation in reinforcement learning (RL), crucial for maximizing cumulative rewards over time. 

<figure style="display: flex; flex-direction: column; align-items: center;">
  <img src="{{ "/assets/img/learning/optimal_actions.png"  | absolute_url }}" alt="mode_shape" class="post-pic" style="width: 70%;"/>
  <figcaption style="text-align: center;">Alice-Bob Optimal Strategy as Defined by a Human</figcaption>
</figure>

We examined the multi-armed bandit problem as a simplified RL setting where the agent must learn to choose actions (or "arms") with the best rewards through action-value methods. The greedy algorithm exploits current knowledge but risks missing better actions, while the epsilon-greedy algorithm introduces random exploration to avoid getting stuck in suboptimal choices. We also explored policy gradients, which optimize policies directly, and the Upper Confidence Bound (UCB) algorithm, which balances exploitation with uncertainty-driven exploration. These algorithms provide different strategies for managing the exploration-exploitation trade-off, each offering distinct advantages depending on the problem.

[Click here to access my GitHub code](https://github.com/YaroKazakov/RL-phd/blob/main/rl_book/code/Chapter2-Multi-armed-Bandits-problems.ipynb)
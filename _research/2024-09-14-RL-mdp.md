---
title:  "Markov Decision Processes - Chapter 3 - Sutton and Barto"
date:   2024-09-14
collection: research
permalink: /research/chapter3_SB
---
In Chapter 3, I explored how Markov Decision Processes (MDPs) extend the framework of bandit problems by incorporating both evaluative feedback and the need for associative actions based on state transitions. 

<figure style="display: flex; flex-direction: column; align-items: center;">
  <img src="{{ "/assets/img/learning/MDP.png"  | absolute_url }}" alt="mode_shape" class="post-pic" style="width: 70%;"/>
  <figcaption style="text-align: center;">Alice-Bob Optimal Strategy as Defined by a Human</figcaption>
</figure>

The core challenge is balancing immediate rewards against delayed rewards, as actions influence future states and, consequently, future rewards. The dynamics of an MDP are defined by transition probabilities p(s′, r∣ s,a), which fully describe the environment’s behavior under the Markov property. I also examined how the return is computed, either in episodic tasks, where there's a terminal state, or continuous tasks, and the significance of discounting rewards using a factor γ. Furthermore, I worked through exercises to devise MDPs in diverse contexts and explored how cumulative rewards shift under changes in reward structures.

[Click here to access my GitHub code](https://github.com/YaroKazakov/RL-phd/blob/main/rl_book/chapter_notes/Chapter3_MDP_notes.pdf)